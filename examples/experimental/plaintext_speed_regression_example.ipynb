{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "\t- Hooking PyTorch\n",
      "\t- Creating Virtual Workers:\n",
      "\t\t- bob\n",
      "\t\t- theo\n",
      "\t\t- jason\n",
      "\t\t- alice\n",
      "\t\t- andy\n",
      "\t\t- jon\n",
      "\tStoring hook and workers as global variables...\n",
      "\tLoading datasets from SciKit Learn...\n",
      "\t\t- Boston Housing Dataset\n",
      "\t\t- Diabetes Dataset\n",
      "\t\t- Breast Cancer Dataset\n",
      "\t- Digits Dataset\n",
      "\t\t- Iris Dataset\n",
      "\t\t- Wine Dataset\n",
      "\t\t- Linnerud Dataset\n",
      "\tDistributing Datasets Amongst Workers...\n",
      "\tCollecting workers into a VirtualGrid...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sy.create_sandbox(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example data**: the correct $\\beta$ is $[1, 2, -1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = th.tensor(10 * np.random.randn(30, 3))\n",
    "y = (X[:, 0] + 2 * X[:, 1] - X[:, 2]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into chunks and send a chunk to each worker, storing pointers to chunks in two `MultiPointerTensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = [alice, bob, theo]\n",
    "crypto_provider = jon\n",
    "chunk_size = int(X.shape[0] / len(workers))\n",
    "\n",
    "def _get_chunk_pointers(data, chunk_size, workers):\n",
    "    return [\n",
    "        data[(i * chunk_size):((i+1)*chunk_size), :].send(worker)\n",
    "        for i, worker in enumerate(workers)\n",
    "    ] \n",
    "\n",
    "X_ptrs = sy.MultiPointerTensor(\n",
    "    children=_get_chunk_pointers(X, chunk_size, workers))\n",
    "y_ptrs = sy.MultiPointerTensor(\n",
    "    children=_get_chunk_pointers(y, chunk_size, workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"big data\" step, and it's performed locally on each worker in plain text. The result is two `MultiPointerTensor`s with pointers to each workers' summand of $X^tX$ (or $X^ty$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_ptrs = X_ptrs.transpose(0, 1)\n",
    "\n",
    "XtX_summand_ptrs = Xt_ptrs.mm(X_ptrs)\n",
    "Xty_summand_ptrs = Xt_ptrs.mm(y_ptrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add those summands up in two steps:\n",
    "- share each summand among all other workers\n",
    "- move the resulting pointers to one place (here just the local worker) and add 'em up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_shared_summand_pointers(\n",
    "        summand_ptrs, \n",
    "        workers, \n",
    "        crypto_provider):\n",
    "\n",
    "    for worker_id, summand_pointer in summand_ptrs.child.items():\n",
    "        shared_summand_pointer = summand_pointer.fix_precision().share(\n",
    "            *workers, crypto_provider=crypto_provider)\n",
    "        yield shared_summand_pointer.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX_shared = sum(\n",
    "    _generate_shared_summand_pointers(\n",
    "        XtX_summand_ptrs, workers, crypto_provider))\n",
    "\n",
    "Xty_shared = sum(_generate_shared_summand_pointers(\n",
    "    Xty_summand_ptrs, workers, crypto_provider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient $\\beta$ is the solution to\n",
    "$$X^t X \\beta = X^t y$$\n",
    "\n",
    "**TODO**: It _should_ be possible to solve by performing additive operations on the `XtX_shared` and `Xty_shared` (seems like Gaussian elimination or LDL decomposition could be implemented using operations that `AdditiveSharingTensor` supports). In that case, we'd perform those operations and reconstruct $\\beta$ on each worker without the local worker having to see any sensitive values.\n",
    "\n",
    "For now, we're going to punt and just move `XtX_shared` and `Xty_shared` to the local worker. So there's one party that's trusted to see $X^t X$ and $X^t y$, which are effectively summary statistics of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = XtX_shared.get().float_precision().inverse().mm(\n",
    "    Xty_shared.get().float_precision())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000],\n",
       "        [ 2.0000],\n",
       "        [-1.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
